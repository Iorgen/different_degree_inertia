{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jurgen/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jurgen/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jurgen/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jurgen/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jurgen/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jurgen/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8526005461188308265\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 2574739499674547786\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import copy\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from random import shuffle, randint\n",
    "\n",
    "from numpy import mean, std, dstack\n",
    "\n",
    "from pandas import read_csv\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import (Input, Dense, Flatten, Dropout, Conv1D, LSTM, GRU,\n",
    "                          TimeDistributed, GlobalAveragePooling1D, MaxPooling1D)\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.optimizers import Adam, RMSprop,SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "%matplotlib inline\n",
    "with pd.option_context(\"display.max_rows\", 10, \"display.max_columns\", 20):\n",
    "    print(pd.get_option(\"display.max_rows\")) \n",
    "    print(pd.get_option(\"display.max_columns\"))\n",
    "    \n",
    "# Check GPU support \n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import chdir as cd\n",
    "cd('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seriem_temporis.display_functions import display_block_of_values\n",
    "from seriem_temporis.controller import SignalController, KasperskySetSignalController\n",
    "from neural_models.models import SplitConvolutionalAnomalyDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 60\n",
      "NaN Values: False\n",
      "Number of columns after clean: 55\n",
      "Successfully scaled control_results\n",
      "savgol filter smoothing successful\n",
      "(2988, 500, 1)\n",
      "anomaly exist\n",
      "(2988, 500, 55)\n",
      "(2988, 500, 2)\n",
      "WARNING:tensorflow:From /home/jurgen/.local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "neural_model = SplitConvolutionalAnomalyDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 60\n",
      "NaN Values: False\n",
      "Number of columns after clean: 55\n",
      "Successfully scaled control_results\n",
      "savgol filter smoothing successful\n",
      "(2988, 500, 1)\n",
      "anomaly exist\n",
      "(2988, 500, 55)\n",
      "(2988, 500, 2)\n",
      "Number of columns: 60\n",
      "NaN Values: False\n",
      "Number of columns after clean: 55\n",
      "Successfully scaled control_results\n",
      "savgol filter smoothing successful\n",
      "(2988, 500, 1)\n",
      "anomaly exist\n",
      "(2988, 500, 55)\n",
      "(2988, 500, 2)\n",
      "Number of columns: 60\n",
      "NaN Values: False\n",
      "Number of columns after clean: 55\n",
      "Successfully scaled control_results\n",
      "savgol filter smoothing successful\n",
      "(2988, 500, 1)\n",
      "anomaly exist\n",
      "(2988, 500, 55)\n",
      "(2988, 500, 2)\n",
      "Number of columns: 60\n",
      "NaN Values: False\n",
      "Number of columns after clean: 55\n",
      "Successfully scaled control_results\n",
      "savgol filter smoothing successful\n",
      "(2988, 500, 1)\n",
      "anomaly exist\n",
      "(2988, 500, 55)\n",
      "(2988, 500, 2)\n",
      "Number of columns: 60\n",
      "NaN Values: False\n",
      "Number of columns after clean: 55\n",
      "Successfully scaled control_results\n",
      "savgol filter smoothing successful\n",
      "(2988, 500, 1)\n",
      "anomaly exist\n",
      "(2988, 500, 55)\n",
      "(2988, 500, 2)\n",
      "Number of columns: 60\n",
      "NaN Values: False\n",
      "Number of columns after clean: 55\n",
      "Successfully scaled control_results\n",
      "savgol filter smoothing successful\n",
      "(2988, 500, 1)\n",
      "(2988, 500, 55)\n",
      "(2988, 500, 2)\n",
      "Number of columns: 60\n",
      "NaN Values: False\n",
      "Number of columns after clean: 55\n",
      "Successfully scaled control_results\n",
      "savgol filter smoothing successful\n",
      "(2988, 500, 1)\n",
      "(2988, 500, 55)\n",
      "(2988, 500, 2)\n"
     ]
    }
   ],
   "source": [
    "neural_model.fit_model(verbose=1, epochs_per_step=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise neural model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "def IOU_score(y_true, y_pred):\n",
    "    IOU = []\n",
    "    for idx, i in tf.map_fn(enumerate(y_pred)):\n",
    "        OL = 0\n",
    "        UN = 0\n",
    "        for _idx, result in enumerate(i):\n",
    "            if result ==1 and y_true[idx,_idx]==1:\n",
    "                OL += 1  \n",
    "            if result ==1 or y_true[idx,_idx]==1:\n",
    "                UN += 1\n",
    "        if OL != 0 and UN != 0:\n",
    "            IOU.append(OL/UN)\n",
    "    return mean(IOU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate model results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_cont.predict([X_test,X_test,X_test])\n",
    "y_anomaly_test = Y_test[:,:,1]\n",
    "y_anomaly_prediction = y_pred[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_threshold = 0.4\n",
    "y_anomaly_prediction = (y_anomaly_prediction > decision_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0]), array([2934000]))\n",
      "(array([0., 1.], dtype=float32), array([2933620,     380]))\n",
      "(1956, 1500)\n",
      "(1956, 1500)\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_anomaly_prediction, return_counts=True))\n",
    "print(np.unique(y_anomaly_test, return_counts=True))\n",
    "print(y_anomaly_prediction.shape)\n",
    "print(y_anomaly_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cummulative_test = y_anomaly_test.reshape(y_anomaly_test.shape[0]*y_anomaly_test.shape[1])\n",
    "cummulative_test_pred = y_anomaly_prediction.reshape(y_anomaly_test.shape[0]*y_anomaly_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2934000,)\n",
      "(2934000,)\n"
     ]
    }
   ],
   "source": [
    "print(cummulative_test.shape)\n",
    "print(cummulative_test_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jurgen/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "_f1_score = f1_score(cummulative_test, cummulative_test_pred)\n",
    "_recall_score = recall_score(cummulative_test, cummulative_test_pred)\n",
    "_precision_score = precision_score(cummulative_test, cummulative_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "print(_f1_score, _recall_score, _precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "IOU = []\n",
    "for idx, i in enumerate(y_anomaly_prediction):\n",
    "    OL = 0\n",
    "    UN = 0\n",
    "    for _idx, result in enumerate(i):\n",
    "        if result ==1 and y_anomaly_test[idx,_idx]==1:\n",
    "            OL += 1  \n",
    "        if result ==1 or y_anomaly_test[idx,_idx]==1:\n",
    "            UN += 1\n",
    "    if OL != 0 and UN != 0:\n",
    "        IOU.append(OL/UN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jurgen/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/jurgen/.local/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "print(mean(IOU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
