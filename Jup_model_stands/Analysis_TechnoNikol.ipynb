{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c92f76d2c077>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ],
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error"
    }
   ],
   "source": [
    "# Подгрузим необходимые для работы библиотеки\n",
    "import glob\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy\n",
    "import copy\n",
    "from random import *\n",
    "from scipy import signal, fftpack\n",
    "from scipy.signal import savgol_filter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from random import random\n",
    "from numpy import array\n",
    "from numpy import cumsum\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "with pd.option_context(\"display.max_rows\", 10, \"display.max_columns\", 20):\n",
    "    print(pd.get_option(\"display.max_rows\")) \n",
    "    print(pd.get_option(\"display.max_columns\"))\n",
    "FN = 1\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import GRU, LSTM\n",
    "from keras.layers import Bidirectional, Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class that save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SOP:\n",
    "    control_results = None\n",
    "    smoothed_control_results = None\n",
    "    hord_shift = 145\n",
    "    pc_shift = 50 \n",
    "    pc_schemes = [3,4,5,6,9,10]\n",
    "    hord_schemes = [1,2,7,8,11,12,13,14,15,16]\n",
    "    transverse_defects_columns = [7,8,15,16]\n",
    "    longitudinal_defects_set = None # Список тактов отслеживающих продольные дефекты\n",
    "    transverse_defects_set = None # Список тактов отслеживающих поперечные дефекты\n",
    "    \n",
    "    def __init__(self, filepath=\"SOP1.txt\"):\n",
    "        self.filepath = filepath\n",
    "        \n",
    "    def read_control_results(self):\n",
    "        # В рамках текущего модуля пока избавимся от первого столбца с номерами измерений, пока в них нет необходимости\n",
    "        splited_info = \"\"\n",
    "        with open(self.filepath, mode=\"r\", encoding=\"cp1251\") as f:\n",
    "            splited_info = f.readline()\n",
    "        self.control_results=pd.read_csv(self.filepath, encoding=\"cp1251\", delimiter = \" \", header = None, skiprows = 1)\n",
    "        self.control_results = self.control_results.drop(columns=[0])\n",
    "        print(self.control_results.shape[0])\n",
    "        splited_info = splited_info.split(' ')\n",
    "        self.date = splitedУВО_info[1]\n",
    "        self.time = splited_info[2]\n",
    "        self.temperature = splited_info[3]\n",
    "        \n",
    "    def fix_shift_issue(self):\n",
    "        self.control_results = self.control_results.iloc[:1024]\n",
    "        self.smoothed_control_results = self.smoothed_control_results.iloc[:1024]\n",
    "        for scheme in self.pc_schemes:\n",
    "            self.control_results[scheme] = np.roll(self.control_results[scheme], -self.pc_shift)\n",
    "            self.smoothed_control_results[scheme] = np.roll(self.smoothed_control_results[scheme], -self.pc_shift)\n",
    "        \n",
    "        for scheme in self.hord_schemes:\n",
    "            self.control_results[scheme] = np.roll(self.control_results[scheme], -self.hord_shift)\n",
    "            self.smoothed_control_results[scheme] = np.roll(self.smoothed_control_results[scheme], -self.hord_shift)\n",
    "    \n",
    "    def smooth_control_results(self):\n",
    "        self.smoothed_control_results = copy.deepcopy(self.control_results)\n",
    "        for cont_res in self.control_results:\n",
    "            self.smoothed_control_results[cont_res] = savgol_filter(self.control_results[cont_res], 15, 3)\n",
    "    \n",
    "    def split_by_defects(self):\n",
    "        self.longitudinal_defects_set = self.smoothed_control_results.drop(self.transverse_defects_columns, axis=1)\n",
    "        self.transverse_defects_set = self.smoothed_control_results[self.transverse_defects_columns]\n",
    "        \n",
    "    def load_target_variable(self, filepath):\n",
    "        target = pd.read_csv(filepath)\n",
    "        self.longitudinal_defects_set['y'] = 0\n",
    "        self.transverse_defects_set['y_t'] = 0\n",
    "        # Fill l defect\n",
    "        for index, row in target[target['Тип дефекта'] == \"L\"].iterrows():\n",
    "            self.longitudinal_defects_set[\"y\"][int(row[\"Начало дефекта\"]):int(row[\"Начало дефекта\"]) + int(row[\"Длина дефекта\"])] = 1\n",
    "\n",
    "        for index, row in target[target['Тип дефекта'] == \"T\"].iterrows():\n",
    "            self.transverse_defects_set[\"y_t\"][int(row[\"Начало дефекта\"]):15 + int(row[\"Начало дефекта\"])] = 1\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sop_list = []\n",
    "sop_files = glob.glob(\"Results/*test.dat\")\n",
    "for file_path in sop_files:\n",
    "    print(file_path)\n",
    "    test_sop_list.append(SOP(file_path))\n",
    "\n",
    "for sop in test_sop_list:\n",
    "    sop.read_control_results()\n",
    "    sop.smooth_control_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload target variables (anomalies) from additional file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sop_list[0].load_target_variable(\"Target_/k16_test.csv\")\n",
    "test_sop_list[1].load_target_variable(\"Target_/k22_test.csv\")\n",
    "test_sop_list[2].load_target_variable(\"Target_/k3_test.csv\")\n",
    "test_sop_list[3].load_target_variable(\"Target_/k28_test.csv\")\n",
    "test_sop_list[4].load_target_variable(\"Target_/k24_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Enginering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh.examples.robot_execution_failures import download_robot_execution_failures, \\\n",
    "    load_robot_execution_failures\n",
    "download_robot_execution_failures()\n",
    "timeseries, y = load_robot_execution_failures()\n",
    "timeseries.info()\n",
    "\n",
    "# need to understand how to do that and fix that code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build LSTM model for animalies detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, LSTM, MaxPooling1D, Conv1D\n",
    "from keras.models import Model\n",
    "model = Sequential()\n",
    "input_layer = Input(shape=(1024, 12))\n",
    "conv1 = Conv1D(filters=32,\n",
    "               kernel_size=8,\n",
    "               strides=1,\n",
    "               activation='relu',\n",
    "               padding='same')(input_layer)\n",
    "lstm1 = Bidirectional(LSTM(16,return_sequences=True))(conv1)\n",
    "output_layer = GRU(1, activation=\"sigmoid\", return_sequences=True)(lstm1)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "# model.add(Bidirectional(LSTM(16, input_shape=(1024, 12), return_sequences=True)))\n",
    "# model.add(Bidirectional(LSTM(16, return_sequences=True)))\n",
    "# model.add(GRU(1, activation=\"sigmoid\", return_sequences=True)) \n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"Adam\",  metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tscv = TimeSeriesSplit(n_splits=4)\n",
    "print(tscv)  \n",
    "\n",
    "for train_index, test_index in tscv.split(train_X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = train_X[train_index], train_X[test_index]\n",
    "    y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=1)\n",
    "    model.evaluate(X_test, y_test, verbose=1)\n",
    "    scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print('binary_crossentropy=%f, accuracy=%f' % (scores[0],scores[1]))\n",
    "    \n",
    "# , validation_split=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}